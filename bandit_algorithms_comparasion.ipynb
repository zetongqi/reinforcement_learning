{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent of np.random.choice with numba sipport\n",
    "@jit(nopython=True)\n",
    "def rand_choice_nb(k, prob):\n",
    "    return np.arange(k)[np.searchsorted(np.cumsum(prob), np.random.random(), side=\"right\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def gradient_bandit(k, env, alpha1, alpha2, seed_id, steps=1000):\n",
    "    '''\n",
    "    params\n",
    "    k: num of arms\n",
    "    env: the environment represented as a tuple: (mean, cov), mean and cov are both k*1 vectors\n",
    "    alpha1: step size for updating H(a)\n",
    "    alpha2: step size for computing average reward R_bar\n",
    "    seed_id: random seed id used to perturb the environment\n",
    "    steps: the number of total time steps in a simulation\n",
    "    '''\n",
    "    \n",
    "    # nonstaionary environment setup\n",
    "    mean, cov = env\n",
    "    \n",
    "    # gradient bandit params initialization\n",
    "    H = np.zeros(k)\n",
    "    R_bar = 0\n",
    "    Pi = np.exp(H) / np.sum(np.exp(H))\n",
    "    \n",
    "    # avg reward per timestep\n",
    "    Rt = np.zeros(steps)\n",
    "    \n",
    "    # sum of rewards\n",
    "    R_sum = 0\n",
    "    \n",
    "    # percentages of optimal actions at each time step\n",
    "    opt_act = np.zeros(steps)\n",
    "    \n",
    "    # total number of optimal actions\n",
    "    opt_act_sum = 0\n",
    "    \n",
    "    for t in range(1, steps+1):\n",
    "        # select an action\n",
    "        a = rand_choice_nb(k, Pi)\n",
    "        \n",
    "        # if an action is optimal action, increment optimal action counter\n",
    "        if mean[a] == np.max(mean):\n",
    "            opt_act_sum += 1\n",
    "        opt_act[t-1] = opt_act_sum / t\n",
    "        \n",
    "        # draw a reward from environment\n",
    "        R = np.random.normal(mean[a], cov[a])\n",
    "        \n",
    "        R_sum += R\n",
    "        Rt[t-1] = R_sum / t\n",
    "        \n",
    "        # update H(a)\n",
    "        for j in range(k):\n",
    "            if j != a:\n",
    "                H[j] -= alpha1 * (R - R_bar) * Pi[j]\n",
    "        H[a] += alpha1 * (R - R_bar) * (1 - Pi[a])\n",
    "        \n",
    "        # update softmax distribution Pi\n",
    "        Pi = np.exp(H) / np.sum(np.exp(H))\n",
    "        \n",
    "        # update average reward R_bar\n",
    "        R_bar += alpha2 * (R - R_bar)\n",
    "        \n",
    "        # perturb the environment using random seed\n",
    "        np.random.seed(seed_id)\n",
    "        mean += np.random.normal(0, 1, k)\n",
    "        print(\"gradient\", mean)\n",
    "            \n",
    "    return Rt, opt_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def ucb_bandit(k, C, alpha, env, seed_id, steps=1000):\n",
    "    '''\n",
    "    params\n",
    "    k: number of arms\n",
    "    C: UCB param\n",
    "    alpah: step size for updating Q (weighted average)\n",
    "    env: the environment represented as a tuple: (mean, cov), mean and cov are both k*1 vectors\n",
    "    seed_id: random seed id used to perturb the environment\n",
    "    steps: the number of total time steps in a simulation\n",
    "    '''\n",
    "    \n",
    "    # environment setup\n",
    "    mean, cov = env\n",
    "    \n",
    "    # params initializations\n",
    "    Q = np.zeros(k)\n",
    "    N = np.zeros(k)\n",
    "    \n",
    "    # average reward at each time step\n",
    "    Rt = np.zeros(steps)\n",
    "    \n",
    "    # sum of reward at each time step\n",
    "    R_sum = 0\n",
    "    \n",
    "    # percentages of optimal actions at each time step\n",
    "    Opt = np.zeros(steps)\n",
    "    \n",
    "    # amount of optimal actions at each time step\n",
    "    Opt_sum = 0\n",
    "    \n",
    "    for t in range(1, steps+1):\n",
    "        # Upper-confidence bound action selection\n",
    "        a = Q + C * np.sqrt(np.log(t) / (N + 1e-16))\n",
    "        max_actions = np.argwhere(a == np.max(a))\n",
    "        At = max_actions[np.random.randint(0, len(max_actions))][0]\n",
    "        \n",
    "        # collect reward from the environment\n",
    "        R = np.random.normal(mean[At], cov[At])\n",
    "        \n",
    "        # record avg reward and percentage of optimal actions\n",
    "        R_sum += R\n",
    "        Rt[t-1] = R_sum / t\n",
    "        if mean[At] == np.max(mean):\n",
    "            Opt_sum += 1\n",
    "        Opt[t-1] = Opt_sum / t\n",
    "        \n",
    "        # update estimates and counts\n",
    "        Q += alpha * (R - Q)\n",
    "        N[At] += 1\n",
    "        \n",
    "        # perturb the environment using random seed\n",
    "        np.random.seed(seed_id)\n",
    "        mean += np.random.normal(0, 1, k)\n",
    "        \n",
    "    return Rt, Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
